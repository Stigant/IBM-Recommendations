{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/2322/view).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>b96a4f2e92d8572034b1e9b28f9ac673765cd074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>06485706b34a5c9bf2a0ecdac41daf7e7654ceb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>f01220c46fc92c6e6b161b1849de11faacd7ccb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1      1314.0       healthcare python streaming application demo   \n",
       "2      1429.0         use deep learning for image classification   \n",
       "3      1338.0          ml optimization using cognitive assistant   \n",
       "4      1276.0          deploy your python model as a restful api   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  \n",
       "2  b96a4f2e92d8572034b1e9b28f9ac673765cd074  \n",
       "3  06485706b34a5c9bf2a0ecdac41daf7e7654ceb7  \n",
       "4  f01220c46fc92c6e6b161b1849de11faacd7ccb2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "2  Here’s this week’s news in Data Science and Bi...   \n",
       "3  Learn how distributed DBs solve the problem of...   \n",
       "4  This video demonstrates the power of IBM DataS...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  \n",
       "2         This Week in Data Science (April 18, 2017)       Live           2  \n",
       "3  DataLayer Conference: Boost the performance of...       Live           3  \n",
       "4      Analyze NY Restaurant data using Spark in DSX       Live           4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show df_content to get an idea of the data\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content.doc_full_name=df_content.doc_full_name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=pd.merge(df_content, df[['article_id', 'title']].drop_duplicates(), left_on='doc_full_name', right_on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.apply(lambda row: row.article_id_x == row.article_id_y, axis=1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The df and df_content dataframes contain differing sets of articles. I had worried the article id's might be wrong but they do match on titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000b6387a0366322d7fbfc6434af145adf7fed1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001055fc0bb67f71e8fa17002342b256a30254cd</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00148e4911c7e04eeff8def7bbbdaf1c59c2c621</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001a852ecbd6cc12ab77a785efa137b2646505fe</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001fc95b90da5c3cb12c501d201a915e4f093290</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Views\n",
       "email                                          \n",
       "0000b6387a0366322d7fbfc6434af145adf7fed1     13\n",
       "001055fc0bb67f71e8fa17002342b256a30254cd      4\n",
       "00148e4911c7e04eeff8def7bbbdaf1c59c2c621      3\n",
       "001a852ecbd6cc12ab77a785efa137b2646505fe      6\n",
       "001fc95b90da5c3cb12c501d201a915e4f093290      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_title=df.drop('title', axis=1)\n",
    "user_view_counts=df_no_title.groupby('email').count()\n",
    "user_view_counts.columns=['Views']\n",
    "article_view_counts=df_no_title.groupby('article_id').count()\n",
    "article_view_counts.columns=['Views']\n",
    "user_view_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Views    3.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.930847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.802267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>364.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Views\n",
       "count  5148.000000\n",
       "mean      8.930847\n",
       "std      16.802267\n",
       "min       1.000000\n",
       "25%       1.000000\n",
       "50%       3.000000\n",
       "75%       9.000000\n",
       "max     364.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(user_view_counts.median())\n",
    "user_view_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Views\n",
       "article_id       \n",
       "0.0            14\n",
       "2.0            58\n",
       "4.0            13\n",
       "8.0            85\n",
       "9.0            10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_view_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id\n",
       "50     True\n",
       "221    True\n",
       "232    True\n",
       "398    True\n",
       "577    True\n",
       "Name: doc_body, dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "dupes=df_content.groupby('article_id').count().iloc[:,0] > 1\n",
    "dupes[dupes].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e8c9a7165a9e>:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  duplicated=df_content.set_index('article_id').sort_index()[dupes]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>During the seven-week Insight Data Engineering...</td>\n",
       "      <td>graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>Community Detection at Scale</td>\n",
       "      <td>graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>One of the earliest documented catalogs was co...</td>\n",
       "      <td>how smart catalogs can turn the big data flood...</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>* United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\n...</td>\n",
       "      <td>When used to make sense of huge amounts of con...</td>\n",
       "      <td>how smart catalogs can turn the big data flood...</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>self-service data preparation with ibm data re...</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>self-service data preparation with ibm data re...</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>Today’s world of data science leverages data f...</td>\n",
       "      <td>using apache spark as a parallel processing fr...</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>Today’s world of data science leverages data f...</td>\n",
       "      <td>using apache spark as a parallel processing fr...</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>use the primary index</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>use the primary index</td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     doc_body  \\\n",
       "article_id                                                      \n",
       "50          Follow Sign in / Sign up Home About Insight Da...   \n",
       "50          Follow Sign in / Sign up Home About Insight Da...   \n",
       "221         Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "221         * United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\n...   \n",
       "232         Homepage Follow Sign in Get started Homepage *...   \n",
       "232         Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "398         Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "398         Homepage Follow Sign in Get started Homepage *...   \n",
       "577         This video shows you how to construct queries ...   \n",
       "577         This video shows you how to construct queries ...   \n",
       "\n",
       "                                              doc_description  \\\n",
       "article_id                                                      \n",
       "50          During the seven-week Insight Data Engineering...   \n",
       "50                               Community Detection at Scale   \n",
       "221         One of the earliest documented catalogs was co...   \n",
       "221         When used to make sense of huge amounts of con...   \n",
       "232         If you are like most data scientists, you are ...   \n",
       "232         If you are like most data scientists, you are ...   \n",
       "398         Today’s world of data science leverages data f...   \n",
       "398         Today’s world of data science leverages data f...   \n",
       "577         This video shows you how to construct queries ...   \n",
       "577         This video shows you how to construct queries ...   \n",
       "\n",
       "                                                doc_full_name doc_status  \n",
       "article_id                                                                \n",
       "50                               graph-based machine learning       Live  \n",
       "50                               graph-based machine learning       Live  \n",
       "221         how smart catalogs can turn the big data flood...       Live  \n",
       "221         how smart catalogs can turn the big data flood...       Live  \n",
       "232         self-service data preparation with ibm data re...       Live  \n",
       "232         self-service data preparation with ibm data re...       Live  \n",
       "398         using apache spark as a parallel processing fr...       Live  \n",
       "398         using apache spark as a parallel processing fr...       Live  \n",
       "577                                     use the primary index       Live  \n",
       "577                                     use the primary index       Live  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated=df_content.set_index('article_id').sort_index()[dupes]\n",
    "print(duplicated.shape)\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n",
      "doc_body           False\n",
      "doc_description    False\n",
      "doc_full_name       True\n",
      "doc_status          True\n",
      "Name: 577, dtype: bool\n",
      "232\n",
      "doc_body           False\n",
      "doc_description     True\n",
      "doc_full_name       True\n",
      "doc_status          True\n",
      "Name: 232, dtype: bool\n",
      "398\n",
      "doc_body           False\n",
      "doc_description     True\n",
      "doc_full_name       True\n",
      "doc_status          True\n",
      "Name: 398, dtype: bool\n",
      "50\n",
      "doc_body           False\n",
      "doc_description    False\n",
      "doc_full_name       True\n",
      "doc_status          True\n",
      "Name: 50, dtype: bool\n",
      "221\n",
      "doc_body           False\n",
      "doc_description    False\n",
      "doc_full_name       True\n",
      "doc_status          True\n",
      "Name: 221, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "for idx in set(duplicated.index):\n",
    "    print(idx)\n",
    "    print(duplicated.loc[idx].iloc[0]==duplicated.loc[idx].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "df_content=df_content.sort_values('article_id').drop_duplicates('article_id',keep='first').set_index('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of viewed articles: 714\n",
      "Total number of articles: (1051, 4)\n",
      "Number of unique users: 5148\n",
      "Number of user_article interactions: (45993, 3)\n"
     ]
    }
   ],
   "source": [
    "#By the numbers\n",
    "print(f'Number of viewed articles: {article_view_counts.shape[0]}')\n",
    "print(f'Total number of articles: {df_content.shape}')\n",
    "print(f'Number of unique users: {user_view_counts.shape[0]}')\n",
    "print(f'Number of user_article interactions: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1429.0</th>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330.0</th>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431.0</th>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427.0</th>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364.0</th>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Views\n",
       "article_id       \n",
       "1429.0        937\n",
       "1330.0        927\n",
       "1431.0        671\n",
       "1427.0        643\n",
       "1364.0        627"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most viewed articles\n",
    "article_view_counts.sort_values('Views', ascending=False).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the email hashes as user ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df.email.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...        1\n",
       "1      1314.0       healthcare python streaming application demo        2\n",
       "2      1429.0         use deep learning for image classification        3\n",
       "3      1338.0          ml optimization using cognitive assistant        4\n",
       "4      1276.0          deploy your python model as a restful api        5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df.loc[:,'user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    article_view_counts=df.groupby('article_id').count().iloc[:,0].copy()\n",
    "    article_view_counts.sort_values(ascending=False,inplace=True)\n",
    "    top_articles=article_view_counts.head(n).index\n",
    "    return list(top_articles) # Return the top article titles from df (not df_content)\n",
    "\n",
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    top_articles=get_top_article_ids(n, df)\n",
    "    top_articles= df.set_index('article_id').loc[top_articles,'title'].drop_duplicates().values\n",
    "        \n",
    "    return list(top_articles) # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    # Fill in the function here\n",
    "    user_item=df.groupby(['user_id', 'article_id']).apply(lambda x: 1).unstack().fillna(0)\n",
    "    \n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_item.shape)\n",
    "print(user_item.sum(axis=1)[1])\n",
    "user_item.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5148, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. \n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest cosine angle)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the cosine angle\n",
    "    Returns an ordered list\n",
    "    If user has no interactions, returns users ordered by similarity between their interactions\n",
    "    and the popularity of articles\n",
    "    \n",
    "    '''\n",
    "    # compute similarity of each user to the provided user\n",
    "    try:\n",
    "        user_views=user_item.loc[user_id].values\n",
    "        user_item=user_item.drop(user_id)\n",
    "    except:\n",
    "        user_views=np.array([])\n",
    "    norms=user_item.apply(norm, axis=1)\n",
    "    total_views=user_item.sum()\n",
    "    \n",
    "    if norm(user_views)==0:\n",
    "        user_views=total_views\n",
    "          \n",
    "    dots=np.dot(user_views,user_item.values.transpose())\n",
    "    user_item['cosine']=dots/(norm(user_views)*norms.values)\n",
    "    # sort by similarity\n",
    "    user_item.sort_values('cosine', inplace=True, ascending=False)\n",
    "    # create list of just the ids\n",
    "    most_similar_users=list(user_item.index)\n",
    "    \n",
    "    return most_similar_users # return a list of the users in order from most to least similar\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    articles=df.set_index('article_id').loc[article_ids,'title'].drop_duplicates()\n",
    "    article_names=list(articles.values)\n",
    "    \n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item, names=True):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    # Your code here\n",
    "    viewed=user_item.loc[user_id,(user_item.loc[user_id]==1)].index\n",
    "    article_ids=list(viewed)\n",
    "    if names:\n",
    "        article_names=get_article_names(article_ids)\n",
    "        return article_ids, article_names # return the ids and names\n",
    "    else:\n",
    "        return article_ids\n",
    "\n",
    "def make_recs_from_similar(similar, m=10):\n",
    "    recs=[]\n",
    "    \n",
    "    while (len(recs) < m or m==-1) and len(similar) > 0:\n",
    "        similar_user=similar.pop()\n",
    "        possible_recs=get_user_articles(similar_user, names=False)\n",
    "        while (len(recs) < m or m==-1) and len(possible_recs) > 0:\n",
    "            rec=possible_recs.pop()\n",
    "            if rec in recs:\n",
    "                continue\n",
    "            else:\n",
    "                recs.append(rec)\n",
    "    return recs\n",
    "\n",
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    similar=find_similar_users(user_id)\n",
    "    recs=make_recs_from_similar(similar, m)\n",
    "    \n",
    "    return recs # return your recommendations for this user_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_user_recs(1, m=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your functions here - No need to change this code - just run this cell\n",
    "assert set(get_article_names([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names([1320.0, 232.0, 844.0])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set([1320.0, 232.0, 844.0])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # Your code here\n",
    "    try:\n",
    "        user_views=user_item.loc[user_id].values\n",
    "        user_item=user_item.drop(user_id)\n",
    "    except:\n",
    "        user_views=np.array([])\n",
    "    \n",
    "    norms=user_item.apply(norm, axis=1)\n",
    "    total_views=user_item.sum()\n",
    "    \n",
    "    if norm(user_views)==0:\n",
    "        user_views=total_views\n",
    "          \n",
    "    dots=np.dot(user_views,user_item.values.transpose())\n",
    "    \n",
    "    user_item['dot_product']=dots\n",
    "    \n",
    "    view_counts=df.groupby('user_id').count().iloc[:,0].copy().drop(user_id)\n",
    "    view_counts.sort_values(ascending=False,inplace=True)\n",
    "    view_counts.name='views'\n",
    "    \n",
    "    neighbors_df=pd.concat([view_counts, user_item['dot_product']], axis=1).sort_values(['dot_product', 'views'], ascending=False)\n",
    "    \n",
    "    return neighbors_df  # Return the dataframe specified in the doc_string\n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "    similar=list(get_top_sorted_users(user_id).index)\n",
    "    recs=make_recs_from_similar(similar,m)\n",
    "    rec_names=get_article_names(recs, df)\n",
    "    \n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_sorted_users(1).iloc[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_sorted_users(131).iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = 3933 # Find the user that is most similar to user 1 (+1 because I dropped NaN emails)\n",
    "user131_10th_sim = 242 #Find the 10th most similar user to user 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below.  You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "new_user_recs =get_top_article_ids(10) # Your recommendations here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(new_user_recs) == set([1314.0,1429.0,1293.0,1427.0,1162.0,1364.0,1304.0,1170.0,1431.0,1330.0]), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations (EXTRA - NOT REQUIRED)</a>\n",
    "\n",
    "Another method we might use to make recommendations is to perform a ranking of the highest ranked articles associated with some term.  You might consider content to be the **doc_body**, **doc_description**, or **doc_full_name**.  There isn't one way to create a content based recommendation, especially considering that each of these columns hold content related information.  \n",
    "\n",
    "`1.` Use the function body below to create a content based recommender.  Since there isn't one right answer for this recommendation tactic, no test functions are provided.  Feel free to change the function inputs if you decide you want to try a method that requires more input values.  The input values are currently set with one idea in mind that you may use to make content based recommendations.  One additional idea is that you might want to choose the most popular recommendations that meet your 'content criteria', but again, there is a lot of flexibility in how you might make these recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import auc\n",
    "from scipy.linalg import subspace_angles\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_data(url):\n",
    "    \"\"\"Extract hostname and path from url if possible\"\"\"\n",
    "    dat=urlparse(url)\n",
    "    if dat.hostname:\n",
    "        return [x for x in [dat.hostname.strip('www.'), dat.path.strip('/')] if x]\n",
    "    else:\n",
    "        dat=urlparse('//'+url)\n",
    "        return [x for x in [dat.hostname.strip('www.'), dat.path.strip('/')] if x]\n",
    "url_pattern = r\"(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\\\".,<>?«»“”‘’]))\"\n",
    "stopWords=stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\" Pos tag to Wordnet tag \"\"\"\n",
    "    tag=tag[0].upper()\n",
    "    if tag == 'J':\n",
    "        return wordnet.ADJ\n",
    "    elif tag == 'V':\n",
    "        return wordnet.VERB\n",
    "    elif tag == 'N':\n",
    "        return wordnet.NOUN\n",
    "    elif tag == 'R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, suffix=''):\n",
    "    \"\"\"Clean and tokenize text, then lemmatize. Label with suffix if given\"\"\"\n",
    "    text = text.lower()\n",
    "\n",
    "    ##urls\n",
    "    text=re.sub(r\"\\[|\\]\", ' ', text) #They break the regex for some reason\n",
    "\n",
    "    matcher= re.compile(url_pattern)\n",
    "    urls=[match.group() for match in matcher.finditer(text)]\n",
    "    for url in urls:\n",
    "        text=text.replace(url, ' ', 1)\n",
    "\n",
    "    url_tokens=sum([extract_url_data(url) for url in urls], [])\n",
    "\n",
    "    ##Remove punctuation and tokenize\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    word_tokens=word_tokenize(text)\n",
    "    ##Lemmatise\n",
    "    word_tokens = [lemmatizer.lemmatize(word,get_wordnet_pos(tag)) for word,tag in pos_tag(word_tokens) if word not in stopWords]\n",
    "    #word_tokens = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stopWords]\n",
    "    return [token+suffix for token in (url_tokens+word_tokens)]\n",
    "\n",
    "def tokenizer(suffix):\n",
    "    '''Tokenize wrapper'''\n",
    "    return (lambda text: quick_tokenize(text,suffix=suffix))\n",
    "\n",
    "def quick_tokenize(text, suffix=''):\n",
    "    \"\"\"Clean and tokenize text, then lemmatize\"\"\"\n",
    "    text = text.lower()\n",
    "\n",
    "    ##Remove punctuation and tokenize\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    word_tokens=word_tokenize(text)\n",
    "    ##Lemmatise\n",
    "    word_tokens = [lemmatizer.lemmatize(word,get_wordnet_pos(tag)) for word,tag in pos_tag(word_tokens) if word not in stopWords]\n",
    "    #word_tokens = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stopWords]\n",
    "    return [token+suffix for token in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(suffix,vect=None):\n",
    "    '''Make feature pipeline for text tokenisation'''\n",
    "    if vect==None:\n",
    "        vect=CountVectorizer(tokenizer=tokenizer(suffix))\n",
    "        \n",
    "    features=Pipeline([('pull_column', FunctionTransformer(lambda df : df['doc'+suffix], validate=False)),\n",
    "                           ('vect', vect)])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(X, suffixes=['_body', '_description', '_full_name']):\n",
    "    '''Text vectoriser + tfidf transform for given suffixes'''\n",
    "    if len(suffixes) > 1:\n",
    "        text_transformer=FeatureUnion([('transformer'+suffix,transformer(suffix)) for suffix in suffixes])\n",
    "    else:\n",
    "        text_transformer=transformer(suffixes[0])\n",
    "        \n",
    "    pipe=Pipeline([('text_vects', text_transformer),\n",
    "                            ('tfidf', TfidfTransformer())])    \n",
    "    X=pipe.fit_transform(X)\n",
    "    return X, pipe\n",
    "\n",
    "tfidf_df, text_transformer=build_transformer(df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_sim_mat=cosine_similarity(tfidf_df)\n",
    "art_sim_df=pd.DataFrame(art_sim_mat, columns=df_content.index, index=df_content.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keyword_transformer(X, suffixes=['_body', '_description'], key_suffix='_full_name'):\n",
    "    '''Build, fit and transform keyword tranformer'''\n",
    "    \n",
    "    \n",
    "    vect=CountVectorizer(tokenizer=tokenizer(key_suffix))\n",
    "\n",
    "    key_transformer=transformer(key_suffix, vect)\n",
    "    \n",
    "    key_transformer.fit(X)\n",
    "    \n",
    "    key_word_transformer=FeatureUnion([('transformer'+key_suffix, key_transformer)]+\n",
    "                                          [('transformer'+suffix,transformer(suffix, vect)) for suffix in suffixes])\n",
    "    \n",
    "\n",
    "    tfidf=TfidfTransformer()\n",
    "    \n",
    "    X_count=key_word_transformer.transform(X)\n",
    "    \n",
    "    X_tfidf=tfidf.fit_transform(X_count)\n",
    "    \n",
    "    pipe=Pipeline([('key_word_vects', key_word_transformer),\n",
    "                            ('tfidf', tfidf)]) \n",
    "    \n",
    "    return X_tfidf, pipe\n",
    "\n",
    "#tfidf_df, text_transformer=build_keyword_transformer(df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Collect user lists\n",
    "known_users=user_item.loc[:,:(df_content.shape[0]-1)].sum(axis=1) > 7 #Find suitable users for validation\n",
    "known_users=list(user_item[known_users].index)\n",
    "users=list(user_item.index) #All users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User similarities for collaborative recommender reimplementation\n",
    "user_sim_mat=cosine_similarity(user_item)\n",
    "user_sim_df=pd.DataFrame(user_sim_mat, columns=users, index=users)\n",
    "user_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sim_df[1].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_angle_recs(seen,sim_df=art_sim_df, agg=lambda x: x.mean(axis=1)):\n",
    "    '''Content recommender based on cosine similarity'''\n",
    "    \n",
    "    unseen_sim=sim_df.drop(seen).loc[:,seen]\n",
    "    agg_sim=agg(unseen_sim).sort_values(ascending=False)\n",
    "    \n",
    "    return agg_sim\n",
    "def make_content_angle_recs(user, n, sim_df=art_sim_df, user_item=user_item, agg=lambda x: x.mean(axis=1)):\n",
    "    '''\n",
    "    Makes content recommendations.\n",
    "    \n",
    "    INPUT:\n",
    "    user- (int) target of recommendations\n",
    "    n - (int) number of recommendations\n",
    "    sim_df - (df) article similarity matrix\n",
    "    user_item - user/item interaction matrix\n",
    "    agg = (row -> float) method for aggregating similarity scores\n",
    "    \n",
    "    OUTPUT:\n",
    "    (list) n content based recommendations for user\n",
    "    \n",
    "    '''\n",
    "    user_item=user_item.reindex(art_sim_df.columns, axis=1).dropna(axis=1)\n",
    "    interactions=user_item.loc[user]\n",
    "    seen=list(interactions[interactions ==1].index)\n",
    "    agg_sim=content_angle_recs(seen, sim_df=sim_df, agg=lambda x: x.mean(axis=1))\n",
    "    \n",
    "    return list(agg_sim.index)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_content_angle_recs(1,10) #Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_recs(user, seen, user_sim_df=user_sim_df, user_item=user_item,\n",
    "                        weight=lambda rank: (1/2)**rank,\n",
    "                        agg=lambda df: df.sum(axis=1)):\n",
    "    '''Colloborative recommender based on user cosine similarity'''\n",
    "    user_sims=user_sim_df[user].sort_values(ascending=False).drop(user)\n",
    "    \n",
    "    user_item=user_item.copy().drop(seen, axis=1)\n",
    "    ranked_seen=[] \n",
    "    \n",
    "    rank=1\n",
    "    for user2 in user_sims.index:\n",
    "        ranked_seen.append((weight(rank)*user_item.loc[user2]))\n",
    "        rank+=1\n",
    "    \n",
    "    rank_df=agg(pd.concat(ranked_seen,axis=1)).sort_values(ascending=False)\n",
    "\n",
    "    return rank_df\n",
    "\n",
    "def make_user_user_recs(user, n, user_sim_df=user_sim_df, user_item=user_item,\n",
    "                        weight=lambda rank: (1/2)**rank,\n",
    "                        agg=lambda df: df.sum(axis=1)):\n",
    "    '''\n",
    "    Makes collobarative recommendations.\n",
    "    \n",
    "    Ranks other users by similarities. User/article interactions are then weighted by user rank and aggregated.\n",
    "    \n",
    "    INPUT:\n",
    "    user- (int) target of recommendations\n",
    "    n - (int) number of recommendations\n",
    "    user_sim_df - (df) user  similarity matrix\n",
    "    user_item - user/item interaction matrix\n",
    "    weight - (int -> float) score contribution from being seen by user of a given similarity rank \n",
    "    agg = (row -> float) method for weighted interactions\n",
    "    OUTPUT:\n",
    "\n",
    "    '''\n",
    "    \n",
    "    interactions=user_item.loc[user]\n",
    "    seen=list(interactions[interactions ==1].index)\n",
    "    \n",
    "    rank_df=user_user_recs(user,seen, user_sim_df, user_item,weight,agg)\n",
    "    return list(rank_df.index)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_user_user_recs(1,10) #Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(sims,weights):\n",
    "    '''Function to combine content similarities with user/user weightings'''\n",
    "    combo=pd.concat([(weights*sims*0.5),(0.5*sims)], axis=1).sum(axis=1)\n",
    "    return combo\n",
    "\n",
    "def user_content_recs(user, seen, user_sim_df=user_sim_df, user_item=user_item,\n",
    "                        art_sim_df=art_sim_df, art_agg=lambda x: x.mean(axis=1),\n",
    "                        user_weight=lambda rank: (1/2)**rank,\n",
    "                        user_agg=lambda df: df.sum(axis=1),\n",
    "                        combine=combine):\n",
    "    \n",
    "    '''Combinded colloborative + content based recommender'''\n",
    "    \n",
    "    content_sims=content_angle_recs(seen,art_sim_df, art_agg)\n",
    "    collab_weights=user_user_recs(user, seen, user_sim_df, user_item,\n",
    "                        weight=user_weight, agg=user_agg)\n",
    "    weighted_sims=combine(content_sims, collab_weights).sort_values(ascending=False)\n",
    "    return weighted_sims\n",
    "\n",
    "def make_user_content_recs(user,n, user_sim_df=user_sim_df, art_sim_df=art_sim_df,\n",
    "                        user_item=user_item, art_agg=lambda x: x.mean(axis=1),\n",
    "                        user_weight=lambda rank: (1/2)**rank,\n",
    "                        user_agg=lambda df: df.sum(axis=1),\n",
    "                        combine=combine):\n",
    "    '''\n",
    "    Makes collobarative + content based recommendations.\n",
    "    \n",
    "    INPUT:\n",
    "    user- (int) target of recommendations\n",
    "    n - (int) number of recommendations\n",
    "    user_sim_df - (df) user similarity matrix\n",
    "    art_sim_df - (df) article similarity matrix\n",
    "    user_item - (df) user/item interaction matrix\n",
    "    art_agg - (row -> float) method for aggregating article similarity scores\n",
    "    user_weight - (int -> float) score contribution from being seen by user of a given similarity rank \n",
    "    user_agg = (row -> float) method for aggregating weighted interactions\n",
    "    combine = ((series,series) -> series) method for combining article_similarities and aggregated weights \n",
    "    OUTPUT:\n",
    "\n",
    "    '''\n",
    "        \n",
    "    interactions=user_item.reindex(art_sim_df.columns, axis=1).dropna(axis=1).loc[user]\n",
    "    seen=list(interactions[interactions ==1].index)\n",
    "    \n",
    "    weighted_sims=user_content_recs(user,seen, user_sim_df, user_item,\n",
    "                        art_sim_df, art_agg, user_weight,\n",
    "                        user_agg,combine)\n",
    "    \n",
    "    return list(weighted_sims.index)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train=user_item.loc[known_users,:(df_content.shape[0]-1)].copy()\n",
    "test={}\n",
    "seed=42\n",
    "\n",
    "for user in known_users:\n",
    "    interactions=user_train.loc[user]\n",
    "    interactions=interactions[interactions==1]\n",
    "\n",
    "    test[user]=list(interactions.sample(frac=0.25, random_state=seed).index)\n",
    "    user_train.loc[user,test[user]]=0\n",
    "    \n",
    "    seed= seed+1\n",
    "    \n",
    "user_sim_mat_train=cosine_similarity(user_train)\n",
    "user_sim_df_train=pd.DataFrame(user_sim_mat_train, columns=user_train.index, index=user_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df, text_transformer=build_transformer(df_content,suffixes=['_description', '_full_name'])\n",
    "art_sim_mat_train=cosine_similarity(tfidf_df)\n",
    "art_sim_df_train=pd.DataFrame(art_sim_mat_train, columns=df_content.index, index=df_content.index)\n",
    "art_sim_df_train=art_sim_df_train.loc[user_train.columns, user_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommender(recommender, test=test):\n",
    "    '''\n",
    "    Evaluates recommender against a test dictionary. Plots and outputs mean recall, precision and f1-scores.\n",
    "    Also prints and outputs precision/recall auc score.\n",
    "    '''\n",
    "    recalls_list=[]\n",
    "    precisions_list=[]\n",
    "\n",
    "    for user in test.keys():   \n",
    "        recs=recommender(user)\n",
    "    \n",
    "        precisions=[]\n",
    "        recalls=[]\n",
    "        is_pos=[rec in test[user] for rec in recs]\n",
    "    \n",
    "        for i in range(1,900):\n",
    "            n_true= sum(is_pos[:i])\n",
    "            precision= n_true/i\n",
    "            recall=n_true/len(test[user])\n",
    "        \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        recalls_list.append(recalls)\n",
    "        precisions_list.append(precisions)\n",
    "    \n",
    "    mean_precisions=np.array([np.mean(x) for x in np.array(precisions_list).transpose()])\n",
    "    mean_recalls=np.array([np.mean(x) for x in np.array(recalls_list).transpose()])\n",
    "    f1_scores=2*(mean_precisions*mean_recalls)/(mean_precisions+mean_recalls)\n",
    "\n",
    "    f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "    \n",
    "    xlim=100\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(range(1,xlim+1), mean_precisions[:xlim])\n",
    "    plt.plot(range(1,xlim+1), mean_recalls[:xlim])\n",
    "    plt.plot(range(1,xlim+1), f1_scores[:xlim])\n",
    "    plt.legend(['Precision', 'Recall', 'f1_score'])\n",
    "    plt.title('Precision, Recall and f1-score vs Number of Predictions')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Predictions')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,1,2)    \n",
    "    plt.plot(mean_recalls, mean_precisions);\n",
    "    plt.title('Precision vs Recall')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "    auc_score=auc(range(1,900), mean_precisions)\n",
    "    print(f'Precision/Number of Predicitions auc score: {auc_score}')\n",
    "          \n",
    "    auc_score=auc(mean_recalls, mean_precisions)\n",
    "    print(f'Precision/Recall auc score: {auc_score}')\n",
    "    return mean_precisions, mean_recalls, f1_scores, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender= lambda user: make_user_user_recs(user,-1, user_sim_df_train, user_train)\n",
    "evaluate_recommender(recommender,test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender =lambda user: make_content_angle_recs(user,-1, art_sim_df_train, user_train)\n",
    "evaluate_recommender(recommender);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have put together your content-based recommendation system, use the cell below to write a summary explaining how your content based recommender works.  Do you see any possible improvements that could be made to your function?  Is there anything novel about your content based recommender?\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write an explanation of your content based recommendation system here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content based recommendation was done by basic tokenization with nltk. A Tfidf was used on the resulting word counts. This was not ideal, the doc_body included a load of unrelated data. It included for example text for log in and search buttons in some entries. Because the content of extraneous data varied from entry to entry it may reduce the accuracy of the similarity scores, I also couldn't see a good way of removing the data because I had no good description of what was relevant and what was not. In the end I opted not to use the doc_body field.\n",
    "\n",
    "One option I explored was to train a count vectoriser on only the title or the title + description entries and then use it to transform the doc_body, the logic being they probably contain all the keywords. This was a slight improvement but there wasn't much in it. Another option might be to train a model to predict whether a given user viewed an article based upon the tfidf matrix, then use this to pull out the most relevant words and only use these.\n",
    "\n",
    "I also rebuilt the collaborative recommender for comparsion. I ordered other users by cosine similarity then used the rankings to perform a weighted sum to scoe the articles. The default was to add a score of (1/2)^(rank-1) to each article a given rank interacted with.\n",
    "\n",
    "To compare performance I firstly restricted attention to articles for which there was relevant data in df_content. I then restricted myself to users with at least 8 interactions and extracted about 25% of their interactions as a test set. I did not drop articles which none of the selected users had interacted with but there were only a few anyway.\n",
    "\n",
    "I did not use the user data of people with only a few interactions to make predictions, but I did include interactions with articles for which there was no df_content data in the similarity measures. \n",
    "\n",
    "The main metric I was interested in was auc of the precision vs recall curve derived from changing the number of predictions.\n",
    "As can be seen from the graphs and corresponding scores, the content model was pretty poor, especially when compared to the collobarative model.\n",
    "\n",
    "I did implement a mixed model, but it was essentially always worse than just using the user similarity method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use your content-recommendation system to make recommendations for the below scenarios based on the comments.  Again no tests are provided here, because there isn't one right answer that could be used to find these content based recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make recommendations for a brand new user\n",
    "\n",
    "\n",
    "# make a recommendations for a user who only has interacted with article id '1427.0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, you will build use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` You should have already created a **user_item** matrix above in **question 1** of **Part III** above.  This first question here will just require that you run the cells to get things set up for the rest of **Part V** of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "del df['Unnamed: 0']\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the matrix\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` In this situation, you can use Singular Value Decomposition from [numpy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) on the user-item matrix.  Use the cell to perform SVD, and explain why this is different than in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the User-Item Matrix Here\n",
    "\n",
    "u, s, vt = np.linalg.svd(user_item_matrix)# use the built in to get the three matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**\n",
    "\n",
    "We have no missing values here - If a user hasn't seen an article that's a valid datapoint. Therefore we can find the svd decomposition from the diagonalisation of MM*, we don't need any kind of gradient descent like in FunkSVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Use the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below: \n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    user_item_train=create_user_item_matrix(df_train)\n",
    "    user_item_test=create_user_item_matrix(df_test) \n",
    "    \n",
    "    test_arts=list(user_item_test.columns)\n",
    "    test_idx=list(user_item_test.index)\n",
    "    \n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=user_item_train.reindex(test_idx).dropna()\n",
    "user_item_val=user_item_test.reindex(A.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_item_val.shape)\n",
    "user_item_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_idx), len(test_arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': c,# letter here, \n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?': a,# letter here, \n",
    "    'How many movies can we make predictions for in the test set?':b, # letter here,\n",
    "    'How many movies in the test set are we not able to make predictions for because of the cold start problem?': d# letter here\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now use the **user_item_train** dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "Use the cells below to explore how well SVD works towards making predictions for recommendations on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SVD on the user_item_train matrix\n",
    "u_train, s_train, vt_train =np.linalg.svd(user_item_train) # fit svd similar to above then use the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids=list(set(user_item_train.index).intersection(set(test_idx)))\n",
    "test_art_ids=list(set(user_item_train.columns).intersection(set(test_arts)))\n",
    "test_rows=[list(user_item_train.index).index(id) for id in test_ids]\n",
    "test_columns=[list(user_item_train.columns).index(id) for id in test_art_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot Accuracy\n",
    "\n",
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))[test_rows][:,test_columns]\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_est, user_item_val)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot Precision, Recall and F1\n",
    "\n",
    "num_latent_feats = np.arange(10,500+10,10)\n",
    "\n",
    "precisions = []\n",
    "recalls=[]\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s_train[:k]), u_train[:, :k], vt_train[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))[test_rows][:,test_columns]\n",
    "    \n",
    "    # compute and store precision and recall\n",
    "    test_pos=(user_item_val==1)\n",
    "    pred_pos=(user_item_est==1)\n",
    "\n",
    "    true_pos=(test_pos & pred_pos).sum().sum()\n",
    "    \n",
    "    \n",
    "    precision= true_pos/(pred_pos.sum().sum())\n",
    "    recall= true_pos/(test_pos.sum().sum())\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n",
    "\n",
    "precisions=np.array(precisions)\n",
    "recalls=np.array(recalls)\n",
    "f1_scores=2*(precisions*recalls)/(precisions+recalls)\n",
    "    \n",
    "plt.plot(num_latent_feats, precisions);\n",
    "plt.plot(num_latent_feats, recalls);\n",
    "plt.plot(num_latent_feats, f1_scores);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Score');\n",
    "plt.title('Precision and Recall vs. Number of Latent Features');\n",
    "plt.legend(['Precision', 'Recall', 'f1_score']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot Precision, Recall and F1 but zoomed in \n",
    "\n",
    "num_latent_feats = np.arange(20,70,1)\n",
    "\n",
    "precisions = []\n",
    "recalls=[]\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s_train[:k]), u_train[:, :k], vt_train[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))[test_rows][:,test_columns]\n",
    "    \n",
    "    # compute and store precision and recall\n",
    "    test_pos=(user_item_val==1)\n",
    "    pred_pos=(user_item_est==1)\n",
    "\n",
    "    true_pos=(test_pos & pred_pos).sum().sum()\n",
    "    \n",
    "    \n",
    "    precision= true_pos/(pred_pos.sum().sum())\n",
    "    recall= true_pos/(test_pos.sum().sum())\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "precisions=np.array(precisions)\n",
    "recalls=np.array(recalls)\n",
    "f1_scores=2*(precisions*recalls)/(precisions+recalls)\n",
    "    \n",
    "plt.plot(num_latent_feats, precisions);\n",
    "plt.plot(num_latent_feats, recalls);\n",
    "plt.plot(num_latent_feats, f1_scores);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Score');\n",
    "plt.title('Precision and Recall vs. Number of Latent Features');\n",
    "plt.legend(['Precision', 'Recall', 'f1_score']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make with any of the above recommendation systems are an improvement to how users currently find articles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response here.**\n",
    "\n",
    "Accuracy on the a training set is unsurprisngly an increasing function in the number of latent features. If we seperate into a testing set and plot accuracy verus latent features we see that accuracy decreases with the number of latent features. This is essentially because the most accurrate prediction we can make is a given user will not see any articles, since proportion of articles they interacted with is vanishingly small. Out of about 11000 possible interactions only about 218 happened in the test data.\n",
    "\n",
    "Accuracy, Recall and f1 score ae much more relevent here. Precision peaks at a bit over 40 latent features and f1_score with it. There is only a small possible improvement in recall to be gained from additional features, so this seems a natural choice of feature number.\n",
    "\n",
    "This method is a very poor indicator of performance however. The choice of train/test split is very strange, and as a result we get very little overlap between test and train data on which we can make meaningful predictions. I also run a test on the split I used for my content based \n",
    "\n",
    "In any case, once we had suitably refined our models we would likely wish to test their real world performance. This could be done through A/B testing. We are probably most interested in the performance on pre-existing users, since we have only really given one method for providing new users with recommendations. \n",
    "\n",
    "Since we have multiple models, some of which we may wish to blend, we may either run multiple experiments in tandem, and make pairwise  or group comparisons, or opt for a more iterative approach. This might look like testing two options, picking the better, and testing it against a new option and so on. The timescale of the experiment might look very different depending on the choices made, and this should be taken into account.\n",
    "\n",
    "It would not really be appropriate here to test against a system with no, or totally random, recommendations. This would likely severely impact the user experience of one group, which would be at best questionably ethical and at worst detrimental to the overall platform. Comparisons against historical data would likely be flawed as there may be novelty bias, or unrelated changes in user behaviour, however such data could be used effectively to assign users to testing groups.\n",
    "\n",
    "For either situation we would need to fix relative metrics in advance. The most natural here is a user-engagement metric, say mean number of articles interacted with. We might also be interested in user retention, perhaps measured by the number of users who interact with at least one article in the final $n$ days. Subsidiary items of interest might focus on the distribution of interactions. IBM might seek for example to increase the number of interactions with less popular articles, either as a goal in its own right or as part of a longer term aim to increase the diversity of articles.\n",
    "\n",
    "Finally, the users of this site are likely more technologically minded than the global average. They may, therefore, be interested in being afforded the opportunity to choose the way recommendations are made themselves, within some preset limits. If a model blending collaborative and content based recommendations was used, the option might be given to chose the relative weighting of each method. The Steam store, for example, offers something along these lines.\n",
    "\n",
    "It would be difficult, I expect, to get a good idea of the effectiveness of such an option in the short term as there would likely be a large novelty factor. Equally there may be objections if such a feature was rolled out long term to only a subset of users, especially if it proved popular. Conversely comparisons between users who engaged with this feature and those who did not would be poor as it may be biased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u_train, s_train, vt_train =np.linalg.svd(user_train)\n",
    "\n",
    "test_size=sum([len(x) for x in test.values()])\n",
    "\n",
    "thresholds=np.arange(0,0.5,0.05)\n",
    "\n",
    "precision_dict={}\n",
    "recall_dict={}\n",
    "f1_dict={}\n",
    "\n",
    "num_latent_feats = np.arange(10,200,10)\n",
    "for k in num_latent_feats:\n",
    "    \n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s_train[:k]), u_train[:, :k], vt_train[:k, :]\n",
    "    precisions = []\n",
    "    recalls=[]\n",
    "    for t in thresholds:\n",
    "        \n",
    "        # take dot product\n",
    "        user_item_est = (np.dot(np.dot(u_new, s_new), vt_new) > t).astype(int)\n",
    "    \n",
    "        recommendation_locs=(user_item_est-user_train)[user_train !=1] ==1\n",
    "    \n",
    "        n_recs=recommendation_locs.sum().sum()\n",
    "        n_correct=user_item.loc[known_users][recommendation_locs].sum().sum()\n",
    "    \n",
    "        # compute and store precision and recall\n",
    "    \n",
    "        if n_recs > 0:\n",
    "            precision= n_correct/n_recs\n",
    "            recall= n_correct\n",
    "        else:\n",
    "            precision = 1\n",
    "            recall = 0\n",
    "    \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    precision_dict[k]=np.array(precisions)\n",
    "    recall_dict[k]=np.array(recalls)/test_size\n",
    "    \n",
    "df_precision=pd.DataFrame(precision_dict, index=thresholds)\n",
    "df_recall=pd.DataFrame(recall_dict, index=thresholds)\n",
    "df_f1=2*df_precision*df_recall/(df_precision+df_recall)\n",
    "\n",
    "auc_dict={}\n",
    "f1_dict={}\n",
    "thresh_dict={}\n",
    "\n",
    "for k in df_recall.columns:\n",
    "    auc_dict[k]=auc(df_recall[k], df_precision[k])\n",
    "    f1_dict[k]=df_f1[k].max()\n",
    "    thresh_dict[k]=df_f1[k][df_f1[k]==f1_dict[k]].index[0]\n",
    "    precision_dict[k]=df_precision.loc[thresh_dict[k],k]\n",
    "    recall_dict[k]=df_recall.loc[thresh_dict[k],k]\n",
    "    \n",
    "results=pd.DataFrame([auc_dict,f1_dict,thresh_dict, precision_dict, recall_dict],\n",
    "                     index=['AUC score', 'Best f1-score', 'Best Threshold', 'Precision', 'Recall']).transpose()\n",
    "\n",
    "f, axs = plt.subplots(2,1,figsize=(12,12))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(results.iloc[:,0]);\n",
    "plt.plot(results.iloc[:,1]);\n",
    "plt.plot(results.iloc[:,2]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Score');\n",
    "plt.title('AUC and F1 vs. Number of Latent Features');\n",
    "plt.legend(['AUC', 'f1_score', 'Best Treshold']);\n",
    "\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(results.iloc[:,3]);\n",
    "plt.plot(results.iloc[:,4]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Score');\n",
    "plt.title('Precision and Recall vs. Number of Latent Features');\n",
    "plt.legend(['Precision', 'Recall']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this split the recommender does pretty well. I expect I'd pick about 110 latent features and a threshold of 0.2. This gives a good mix of Precision and recall and corresponds to the highest f1-score.\n",
    "\n",
    "These kinds of methods could be combined with the content and collobarative methods from before. If we swapped to FunkSVD in particular we could make some recommendations, set the correspondng entries to NaN and then perform and SVD decomposition with these values missing. This could then be used to make recommendations against a threshold as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conclusions'></a>\n",
    "### Extras\n",
    "Using your workbook, you could now save your recommendations for each user, develop a class to make new predictions and update your results, and make a flask app to deploy your results.  These tasks are beyond what is required for this project.  However, from what you learned in the lessons, you certainly capable of taking these tasks on to improve upon your work here!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "> Congratulations!  You have reached the end of the Recommendations with IBM project! \n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the [rubric](https://review.udacity.com/#!/rubrics/2322/view). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
